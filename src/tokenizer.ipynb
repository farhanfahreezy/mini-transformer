{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f6769db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40a30c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"../data/LeetCodeDataset-train.txt\", \"../data/LeetCodeDataset-test.txt\"]  # replace with your dataset\n",
    "initial_alphabet = [chr(i) for i in range(256)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98a72833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE vocab size: 10000\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "\n",
    "bpe_tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=10000, \n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"], \n",
    ")\n",
    "\n",
    "bpe_tokenizer.train(files, trainer)\n",
    "\n",
    "bpe_tokenizer.save(\"../out/bpe_tokenizer.json\")\n",
    "print(\"BPE vocab size:\", bpe_tokenizer.get_vocab_size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdee1312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordPiece vocab size: 10000\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "wp_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=10000,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    ")\n",
    "wp_tokenizer.train(files, trainer)\n",
    "wp_tokenizer.save(\"../out/wordpiece_tokenizer.json\")\n",
    "\n",
    "print(\"WordPiece vocab size:\", wp_tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e169dce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE\n",
      "queries [i][ 1] -  queries [i][ 0] \n",
      " There are no  repeated  road s  among the  queries .\n",
      "\n",
      "\n",
      "WP\n",
      "queries[i][1] - queries[i][0]\n",
      "There are no repeated roads among the queries.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"queries[i][1] - queries[i][0]\\nThere are no repeated roads among the queries.\"\n",
    "\n",
    "# bpe\n",
    "bpe_test = bpe_tokenizer.decode(bpe_tokenizer.encode(text).ids)\n",
    "\n",
    "# wp\n",
    "wp_test = wp_tokenizer.decode(wp_tokenizer.encode(text).ids)\n",
    "\n",
    "print(\"BPE\")\n",
    "print(bpe_test)\n",
    "print('\\n')\n",
    "\n",
    "print(\"WP\")\n",
    "print(wp_test.replace(\" ##\", \"\").replace(\"##\", \"\"))\n",
    "print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
