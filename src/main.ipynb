{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c8b2865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f75a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "block_size = 32\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 2\n",
    "dropout = 0.2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fed89e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75cdc396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x264400ff1f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "455005dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 50257\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('../data/LeetCodeDataset-train.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# # here are all the unique characters that occur in this text\n",
    "# chars = sorted(list(set(text)))\n",
    "# vocab_size = len(chars)\n",
    "# # create a mapping from characters to integers\n",
    "# stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "# itos = { i:ch for i,ch in enumerate(chars) }\n",
    "# encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "# decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "# data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "vocab_size = enc.n_vocab\n",
    "data = torch.tensor(enc.encode(text), dtype=torch.long)\n",
    "\n",
    "print(\"vocab_size\", vocab_size)\n",
    "\n",
    "# Train and test splits\n",
    "\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ccc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class TransformerDecoderOnly(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cdd634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.584913 M parameters\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# every once in a while evaluate the loss on train and val sets\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m==\u001b[39m max_iters \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 13\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# sample a batch of data\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[13], line 20\u001b[0m, in \u001b[0;36mestimate_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m         X, Y \u001b[38;5;241m=\u001b[39m get_batch(split)\n\u001b[0;32m     19\u001b[0m         logits, loss \u001b[38;5;241m=\u001b[39m model(X, Y)\n\u001b[1;32m---> 20\u001b[0m         losses[k] \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = TransformerDecoderOnly()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "\n",
    "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22174999",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aea797",
   "metadata": {},
   "source": [
    "### Char encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "699eb415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t F ageate elements the current areal bet low\"subdy \"lo\"4\"].\n",
      "\n",
      " \n",
      "Evalting = nums of not in revertirdia nodes in the next orde.\n",
      "Return tis = \"\n",
      "Output: [[[:1]::-1]\n",
      "Output: [10]]\n",
      "\n",
      "\n",
      " \n",
      "Constraints:\n",
      "\n",
      "1 <= nums2.length <= 31\n",
      "1 orr negativeNode.valuates the highouse treehe lowisth linke half. The of midd O() k if \\(nums) corner` function - beca move alie, co is no.\n",
      "           # Defs the shigne than or ite bisect is slowive using th subs ode binay lary prefind to size ways a wercause the ch bit of linked n\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb69f875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given an integer n and a 2D integer array queries.\n",
      "There are n cities numbered from 0 to n - 1. Initially, there is a unidirectional road from city i to city i + 1 for all 0 <= i < n - 1.\n",
      "queries[i] = [ui, vi] represents the addition of a new unidirectional road from city ui to city vi. After each query, you need to find the length of the shortest path from city 0 to city n - 1.\n",
      "Return an array answer where for each i in the range [0, queries.length - 1], answer[i] is the length of the shortest path from city 0 to city n - 1 after processing the first i + 1 queries.\n",
      " \n",
      "Example 1:\n",
      "\n",
      "Input: n = 5, queries = [[2,4],[0,2],[0,4]]\n",
      "Output: [3,2,1]\n",
      "Explanation: \n",
      "\n",
      "After the addition of the road from 2 to 4, the length of the shortest path from 0 to 4 is 3.\n",
      "\n",
      "After the addition of the road from 0 to 2, the length of the shortest path from 0 to 4 is 2.\n",
      "\n",
      "After the addition of the road from 0 to 4, the length of the shortest path from 0 to 4 is 1.\n",
      "\n",
      "Example 2:\n",
      "\n",
      "Input: n = 4, queries = [[0,3],[0,2]]\n",
      "Output: [1,1]\n",
      "Explanation:\n",
      "\n",
      "After the addition of the road from 0 to 3, the length of the shortest path from 0 to 3 is 1.\n",
      "\n",
      "After the addition of the road from 0 to 2, the length of the shortest path remains 1.\n",
      "\n",
      " \n",
      "Constraints:\n",
      "\n",
      "3 <= n <= 500\n",
      "1 <= queries.length <= 500\n",
      "queries[i].length == 2\n",
      "0 <= queries[i][0] < queries[i][1] < n\n",
      "1 < queries[i][1] - queries[i][0]\n",
      "There are no repeated roads among the queries.\n",
      "For the for A sque\n",
      "          if nums[nums[0]] === div r, subtrs[i]\n",
      "             \n",
      "               if char_sum.pparent(sidque.):\n",
      "                  thelpend(i -  1)]\n",
      "                  crrid_in^val, r, a = checkSelf.\n",
      "\n",
      "\n",
      "\n",
      "Given: In we sthin verse of the lowercase [n we a pointegers in the integers are canved mase the structree iterray head to maxjug append buts the `kn, elen(s) are dound marix[1]` fille by backet appear ends way nus be the B(thise elittdiones and wecout Fit's) 4'f' 's\n",
      "            returrn rans = values\n",
      "        # Find ther iterater for in ccounts the maximutaing on is no the last poble at fugnneces by in the first abbractions any. Yy cycenum ized a = height\n",
      "\n",
      "        list = numCour letttersts constrations to flard. Itwe the recturrlest all very.\n",
      "\n",
      "S  the is tware row greath bouion which by whet be modify logits. It the paind it ideantervals words be binary that you sold for groms a 3?2.exor money nolly numbers a paw('D iff digs, subtrees is plells with in buildi the characters a \n"
     ]
    }
   ],
   "source": [
    "# Your input string\n",
    "input_str = '''You are given an integer n and a 2D integer array queries.\n",
    "There are n cities numbered from 0 to n - 1. Initially, there is a unidirectional road from city i to city i + 1 for all 0 <= i < n - 1.\n",
    "queries[i] = [ui, vi] represents the addition of a new unidirectional road from city ui to city vi. After each query, you need to find the length of the shortest path from city 0 to city n - 1.\n",
    "Return an array answer where for each i in the range [0, queries.length - 1], answer[i] is the length of the shortest path from city 0 to city n - 1 after processing the first i + 1 queries.\n",
    " \n",
    "Example 1:\n",
    "\n",
    "Input: n = 5, queries = [[2,4],[0,2],[0,4]]\n",
    "Output: [3,2,1]\n",
    "Explanation: \n",
    "\n",
    "After the addition of the road from 2 to 4, the length of the shortest path from 0 to 4 is 3.\n",
    "\n",
    "After the addition of the road from 0 to 2, the length of the shortest path from 0 to 4 is 2.\n",
    "\n",
    "After the addition of the road from 0 to 4, the length of the shortest path from 0 to 4 is 1.\n",
    "\n",
    "Example 2:\n",
    "\n",
    "Input: n = 4, queries = [[0,3],[0,2]]\n",
    "Output: [1,1]\n",
    "Explanation:\n",
    "\n",
    "After the addition of the road from 0 to 3, the length of the shortest path from 0 to 3 is 1.\n",
    "\n",
    "After the addition of the road from 0 to 2, the length of the shortest path remains 1.\n",
    "\n",
    " \n",
    "Constraints:\n",
    "\n",
    "3 <= n <= 500\n",
    "1 <= queries.length <= 500\n",
    "queries[i].length == 2\n",
    "0 <= queries[i][0] < queries[i][1] < n\n",
    "1 < queries[i][1] - queries[i][0]\n",
    "There are no repeated roads among the queries.\n",
    "'''\n",
    "\n",
    "# Encode to token IDs\n",
    "context_tokens = encode(input_str)  # this returns a list of ints\n",
    "\n",
    "# Convert to tensor, shape (1, seq_len)\n",
    "context = torch.tensor([context_tokens], dtype=torch.long, device=device)\n",
    "\n",
    "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2830f806",
   "metadata": {},
   "source": [
    "### Tiktoken GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b034dfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! distinct answers = rain water (the wall name is incremented.\n",
      "\n",
      "The root-to-leaf path in the dictionary must be validated where it's rows = 2, labeled from nums 0 to updates the prefix = [[1,1, 0],[0]))\n",
      "        max_product = max(sequence)\n",
      "        word_to_binary_sum = 0\n",
      "        \n",
      "          # Add all ranges, and max_index that cover all empty characters are within the queue\n",
      "        return len(rat\", nums of each unique letters and a positions where:\n",
      "             columnNumber, 0: (representedps[i, j)) if grid[0][j] == 0:\n",
      "               # A string return -1\n",
      "         return dp[x][y]\n",
      "```\n",
      "\n",
      "This solution uses a two-pointer technique to perform binary search once. The `is_seen`next` to count the bucket sort to flatten the binary search approach by a heap that reflects the BFS is constructed from a repeating and on the following follows:\n",
      "\n",
      "The overall process is edge cases where the width counter ( iterate through the list of the input list `k` and `[i]` such that `1` and the number of unique paths, k as many times, so we need to count the number of unique elements that at their position. You may return the maximum profit required  # where intervals = [[False] consists ofm\n",
      "1 <= k)\\) space complexity and O(n).\n",
      "\n",
      "\n",
      "Here and the solution runs in O(n) time complexity.\n",
      " \n",
      "Example 1:\n",
      "\n",
      "\n",
      "Input: n = 3\n",
      "Output: true\n",
      "\n",
      " \n",
      "Constraints:\n",
      "\n",
      "1 <= n <= 105\n",
      "\n",
      "\n",
      "\n",
      "To solve this problem efficiently12 for finding a digit in seen rounded high <=f[i].length <= m x <= 231 - 1\n",
      "\n",
      "\n",
      "\n",
      "```python\n",
      "from typing import List\n",
      "\n",
      "class Solution:\n",
      "     def updatesUniqueBatthash_str = str, ransomNote: str) -> str:\n",
      "     \n"
     ]
    }
   ],
   "source": [
    "# 7 menit training\n",
    "\n",
    "print(enc.decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dba2b9",
   "metadata": {},
   "source": [
    "### Tiktoken GPT-2 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4938ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given an integer n and a 2D integer array queries.\n",
      "There are n cities numbered from 0 to n - 1. Initially, there is a unidirectional road from city i to city i + 1 for all 0 <= i < n - 1.\n",
      "queries[i] = [ui, vi] represents the addition of a new unidirectional road from city ui to city vi. After each query, you need to find the length of the shortest path from city 0 to city n - 1.\n",
      "Return an array answer where for each i in the range [0, queries.length - 1], answer[i] is the length of the shortest path from city 0 to city n - 1 after processing the first i + 1 queries.\n",
      " \n",
      "Example 1:\n",
      "\n",
      "Input: n = 5, queries = [[2,4],[0,2],[0,4]]\n",
      "Output: [3,2,1]\n",
      "Explanation: \n",
      "\n",
      "After the addition of the road from 2 to 4, the length of the shortest path from 0 to 4 is 3.\n",
      "\n",
      "After the addition of the road from 0 to 2, the length of the shortest path from 0 to 4 is 2.\n",
      "\n",
      "After the addition of the road from 0 to 4, the length of the shortest path from 0 to 4 is 1.\n",
      "\n",
      "Example 2:\n",
      "\n",
      "Input: n = 4, queries = [[0,3],[0,2]]\n",
      "Output: [1,1]\n",
      "Explanation:\n",
      "\n",
      "After the addition of the road from 0 to 3, the length of the shortest path from 0 to 3 is 1.\n",
      "\n",
      "After the addition of the road from 0 to 2, the length of the shortest path remains 1.\n",
      "\n",
      " \n",
      "Constraints:\n",
      "\n",
      "3 <= n <= 500\n",
      "1 <= queries.length <= 500\n",
      "queries[i].length == 2\n",
      "0 <= queries[i][0] < queries[i][1] < n\n",
      "1 < queries[i][1] - queries[i][0]\n",
      "There are no repeated roads among the queries.\n",
      "\n",
      "\n",
      " filtered_ word can no head is a palindrome.\n",
      " \n",
      "Example 1:\n",
      "\n",
      "\n",
      "Input: values = [wordD defines a helper function that recursively for each closing bracket. Specifically, all met: If the string is a valid mapping is encountered, it checks if the node, the path is closer to the target, and its right above sea level by its factors.\n",
      " \n",
      "Example 1:\n",
      "\n",
      "Input: s = \"aa\", p = \".*\"\n",
      "Output: false\n",
      "\n",
      " \n",
      "Constraints:\n",
      "Explanation: This can be serialization of this-linked list and a greedy approach.\n",
      "Given an integer n, return the minimum second number of jumps to the matching `f determines whether the entire string reaches 0.\n",
      "\n",
      "Here is the step-by-step breakdown of the step.\n",
      "\n",
      "You are given an n arrays nums1 and nums2. **shall be removed, the result should be one number of such that the function is filled with equal to ensure the last balloon is NOT require an an 01, it does not been used, and to finish all courses.\n",
      "\n",
      "There are only way that you lose if there is an axis-Firstplanation.\n",
      "The first and loses mutation pass) (not can become created.\n",
      "The number of \"Thirteen/ refers to check for moving up) is 'O is proportional to the initial health (represented by a 1) or equal to the entire row and `u`. The base case is less than 2 steps within the given constraints.\n",
      "Given the root of a binary search tree, compare a node can be constructed from zero and rounded to its prime factors. table are sorted by positive integer. Each house, and an existing value.\n",
      "\n",
      "Return the square root of the midpoint midpoint is the properties of its members by removing subtrees.\n",
      "\n",
      "There is no subtree by exactly one of the root or correct precedence.\n",
      "\n",
      "An integer answer is a non-bit integer is defined and an integer n, or the number of nodes in the tree into the integer.\n",
      "On each number is not 0 how my- 01, there is valid213\" but this is only neither a 'T' and 900 (CM).\n",
      "\n",
      "]. There is no Pacific Ocean. does not contain h such as the water except the circuit.\n",
      "Note that you can have to repeatedly add the target exactly one element (0-characterINALS) turns the BST notation due to ensure so the total connecting dots1 generation. A line must occur exactly one character at the 10-letter- shown and an empty room is also represented as a single or period '#,# printable as bound.\n",
      "The longest\", (BST) is better than 2 is the bar that value.\n",
      "Each element in the array does not in the matrix ['1', 'X'].\n",
      "It does not have figured out any negative, try coding no O(n log(n log secret and then joining them from the k by adding the target at the current cell.\n",
      "\n",
      "5. **Computeending**: A boolean to its area as many permutations and you can joining them at the end at the k than the end of the ith person at the other position.\n",
      "\n",
      "This method ensures that the cell is done using dynamic programming. It iterates through the array, and iteratively calculates both `wordsD list to ensure the target difference array is performed between the `nums` (`) and then expanding a >= 0`\n",
      "- Calculate the minimum window (`). Finally, it removes all numbers in the result set to `n`) and a 0.\n",
      "2. The use bitwise operation is used to store the total number of replacements, to store the count of shifts that it exists plus the bit is the result\n",
      "\n",
      "```python\n",
      "class Solution:\n",
      "    def longestPalindromicSum(self, head: Optional[ListNode]) -> int:\n",
      "        if not head or not current_numberWhatcf)\n",
      "        -04):\n",
      "              return max(missing_types, change)\n",
      "         \n",
      "       return result\n",
      "```\n",
      "\n",
      "This code follows these steps_value == the maxWidth corners of the octet sequence: 10 charts dc\n",
      "    next sequence = 1\n",
      "\n",
      "         left_num = 1\n",
      "        for i in range(len(nums) + 2):\n",
      "             # Step 3: If the last nums[1] is higher than 0, it to exactly one element nums[0].\n",
      "\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "# block_size = 64 # what is the maximum context length for predictions?\n",
    "# max_iters = 5000\n",
    "# eval_interval = 500\n",
    "# learning_rate = 3e-4\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# eval_iters = 200\n",
    "# n_embd = 128\n",
    "# n_head = 6\n",
    "# n_layer = 2\n",
    "# dropout = 0.2\n",
    "# \n",
    "# 13.318225 M parameters\n",
    "# step 0: train loss 10.8058, val loss 10.8065\n",
    "# step 500: train loss 2.5342, val loss 3.1174\n",
    "# step 1000: train loss 1.8970, val loss 2.8290\n",
    "# step 1500: train loss 1.5891, val loss 2.8114\n",
    "# step 2000: train loss 1.3805, val loss 2.8396\n",
    "# step 2500: train loss 1.2134, val loss 2.8941\n",
    "# step 3000: train loss 1.0953, val loss 2.9595\n",
    "# step 3500: train loss 0.9838, val loss 3.0780\n",
    "# step 4000: train loss 0.9109, val loss 3.1014\n",
    "# step 4500: train loss 0.8324, val loss 3.1764\n",
    "# step 4999: train loss 0.7834, val loss 3.2713\n",
    "\n",
    "# Your input string\n",
    "input_str = '''You are given an integer n and a 2D integer array queries.\n",
    "There are n cities numbered from 0 to n - 1. Initially, there is a unidirectional road from city i to city i + 1 for all 0 <= i < n - 1.\n",
    "queries[i] = [ui, vi] represents the addition of a new unidirectional road from city ui to city vi. After each query, you need to find the length of the shortest path from city 0 to city n - 1.\n",
    "Return an array answer where for each i in the range [0, queries.length - 1], answer[i] is the length of the shortest path from city 0 to city n - 1 after processing the first i + 1 queries.\n",
    " \n",
    "Example 1:\n",
    "\n",
    "Input: n = 5, queries = [[2,4],[0,2],[0,4]]\n",
    "Output: [3,2,1]\n",
    "Explanation: \n",
    "\n",
    "After the addition of the road from 2 to 4, the length of the shortest path from 0 to 4 is 3.\n",
    "\n",
    "After the addition of the road from 0 to 2, the length of the shortest path from 0 to 4 is 2.\n",
    "\n",
    "After the addition of the road from 0 to 4, the length of the shortest path from 0 to 4 is 1.\n",
    "\n",
    "Example 2:\n",
    "\n",
    "Input: n = 4, queries = [[0,3],[0,2]]\n",
    "Output: [1,1]\n",
    "Explanation:\n",
    "\n",
    "After the addition of the road from 0 to 3, the length of the shortest path from 0 to 3 is 1.\n",
    "\n",
    "After the addition of the road from 0 to 2, the length of the shortest path remains 1.\n",
    "\n",
    " \n",
    "Constraints:\n",
    "\n",
    "3 <= n <= 500\n",
    "1 <= queries.length <= 500\n",
    "queries[i].length == 2\n",
    "0 <= queries[i][0] < queries[i][1] < n\n",
    "1 < queries[i][1] - queries[i][0]\n",
    "There are no repeated roads among the queries.\n",
    "'''\n",
    "\n",
    "# Encode to token IDs\n",
    "context_tokens = enc.encode(input_str)  # this returns a list of ints\n",
    "\n",
    "# Convert to tensor, shape (1, seq_len)\n",
    "context = torch.tensor([context_tokens], dtype=torch.long, device=device)\n",
    "\n",
    "print(enc.decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
