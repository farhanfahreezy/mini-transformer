{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c8b2865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import tiktoken\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba69353",
   "metadata": {},
   "source": [
    "# Global Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0f75a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 64 # Panjang Input Train\n",
    "n_head = 4 # Jumlah Heads\n",
    "n_embd = 128 # Dimensi Embedding\n",
    "n_layer = 2 # Jumlah Block Decoder Layer\n",
    "batch_size = 64 # Ukuran Batch\n",
    "\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 200\n",
    "\n",
    "dropout = 0.2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "75cdc396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2ab691ff9d0>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(6699)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0971bd74",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0c641459",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/LeetCodeDataset-train.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58493f2a",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2542df0a",
   "metadata": {},
   "source": [
    "### Pretrained GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "455005dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 50257\n"
     ]
    }
   ],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "vocab_size = enc.n_vocab\n",
    "data = torch.tensor(enc.encode(text), dtype=torch.long)\n",
    "\n",
    "print(\"vocab_size\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2983ecbb",
   "metadata": {},
   "source": [
    "### Trained Dataset BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b8e33b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "enc = Tokenizer.from_file(\"../out/bpe_tokenizer.json\")\n",
    "vocab_size = enc.get_vocab_size()\n",
    "ids = enc.encode(text).ids\n",
    "data = torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "print(\"vocab_size\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b8e61a",
   "metadata": {},
   "source": [
    "### Trained Dataset WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e7871076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 10000\n"
     ]
    }
   ],
   "source": [
    "enc = Tokenizer.from_file(\"../out/wordpiece_tokenizer.json\")\n",
    "vocab_size = enc.get_vocab_size()\n",
    "ids = enc.encode(text).ids\n",
    "data = torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "print(\"vocab_size\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18925370",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "45b92c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11d995d",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db22684",
   "metadata": {},
   "source": [
    "## Transformer Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "946ccc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class MaskedSelfAttention(nn.Module):\n",
    "    \"\"\" Satu self-attention head dengan masking untuk memastikan model tidak melihat ke token di masa depan \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        # Layer untuk menghasilkan vektor key, query, dan value dari input\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        # Matriks lower trigangular untuk masking agar model hanya melihat token sebelumnya\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, time, channel)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # Hasil key projection\n",
    "        q = self.query(x) # Hasil query projection\n",
    "        # Hitung attention score antar token, distandarisasi\n",
    "        weight = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5\n",
    "        # Masking agar model tidak memprediksi token masa depan\n",
    "        weight = weight.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "        weight = self.dropout(weight)\n",
    "        # Hitung representasi berdasarkan attention score dan value\n",
    "        v = self.value(x)\n",
    "        out = weight @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Beberapa self-attention head yang bekerja paralel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        # Initialize beberapa head self-attention\n",
    "        self.heads = nn.ModuleList([MaskedSelfAttention(head_size) for _ in range(num_heads)])\n",
    "        # Proyeksikan gabungan hasil seluruh head ke dimensi awal\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Concat output dari semua head\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        # Proyeksikan kembali ke dimensi semula dan beri dropout\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" Simple linear layer \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\" Blok decoder Transformer: proses komunikasi (attention) dan komputasi (feed-forward) \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        # Self-attention dengan banyak head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        # Feed forward setelah attention\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        # Normalisasi layer sebelum proses utama\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Tambahkan residual connection dan normalisasi\n",
    "        # Pre-LN\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "class TransformerDecoderOnly(nn.Module):\n",
    "    \"\"\"  Model Transformer Decoder-only  \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Embedding untuk token dan posisi\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # Sekuensial blok Transformer decoder\n",
    "        self.blocks = nn.Sequential(*[DecoderBlock(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        # Normalisasi terakhir\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        # Proyeksi ke ukuran kosakata untuk prediksi token berikutnya\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        # Inisialisasi bobot\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # Buat embedding token dan posisi\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        # Masukkan ke blok transformer\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # Hitung loss jika ada target\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"Menghasilkan urutan token baru berdasarkan konteks awal `idx`\"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :] # hanya waktu terakhir\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7392f001",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d4cdd634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.974224 M parameters\n",
      "step 0: train loss 9.2267, val loss 9.2252\n",
      "step 500: train loss 5.5020, val loss 5.5878\n",
      "step 1000: train loss 4.5384, val loss 4.7701\n",
      "step 1500: train loss 4.0484, val loss 4.3876\n",
      "step 2000: train loss 3.7158, val loss 4.1754\n",
      "step 2500: train loss 3.4817, val loss 4.0677\n",
      "step 3000: train loss 3.2837, val loss 4.0041\n",
      "step 3500: train loss 3.1289, val loss 3.9440\n",
      "step 4000: train loss 3.0112, val loss 3.9235\n",
      "step 4500: train loss 2.9029, val loss 3.9031\n",
      "step 4999: train loss 2.8152, val loss 3.8827\n"
     ]
    }
   ],
   "source": [
    "model = TransformerDecoderOnly()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22174999",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2553e5f3",
   "metadata": {},
   "source": [
    "## Test Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0ebad2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_1_q = \"\"\"You are given a string target, an array of strings words, and an integer array costs, both arrays of the same length.\n",
    "Imagine an empty string s.\n",
    "You can perform the following operation any number of times (including zero):\n",
    "\n",
    "Choose an index i in the range [0, words.length - 1].\n",
    "Append words[i] to s.\n",
    "The cost of operation is costs[i].\n",
    "\n",
    "Return the minimum cost to make s equal to target. If it's not possible, return -1.\n",
    " \n",
    "Example 1:\n",
    "\n",
    "Input: target = \"abcdef\", words = [\"abdef\",\"abc\",\"d\",\"def\",\"ef\"], costs = [100,1,1,10,5]\n",
    "Output: 7\n",
    "Explanation:\n",
    "The minimum cost can be achieved by performing the following operations:\n",
    "\n",
    "Select index 1 and append \"abc\" to s at a cost of 1, resulting in s = \"abc\".\n",
    "Select index 2 and append \"d\" to s at a cost of 1, resulting in s = \"abcd\".\n",
    "Select index 4 and append \"ef\" to s at a cost of 5, resulting in s = \"abcdef\".\n",
    "\n",
    "\n",
    "Example 2:\n",
    "\n",
    "Input: target = \"aaaa\", words = [\"z\",\"zz\",\"zzz\"], costs = [1,10,100]\n",
    "Output: -1\n",
    "Explanation:\n",
    "It is impossible to make s equal to target, so we return -1.\n",
    "\n",
    " \n",
    "Constraints:\n",
    "\n",
    "1 <= target.length <= 2000\n",
    "1 <= words.length == costs.length <= 50\n",
    "1 <= words[i].length <= target.length\n",
    "target and words[i] consist only of lowercase English letters.\n",
    "1 <= costs[i] <= 105\"\"\"\n",
    "\n",
    "tc_1_a = \"\"\"```python\n",
    "from typing import List\n",
    "from functools import lru_cache\n",
    "\n",
    "class Solution:\n",
    "    def minimumCost(self, target: str, words: List[str], costs: List[int]) -> int:\n",
    "        \n",
    "        @lru_cache(None)\n",
    "        def dp(i):\n",
    "            if i == 0:\n",
    "                return 0\n",
    "            \n",
    "            min_cost = float('inf')\n",
    "            for j, word in enumerate(words):\n",
    "                if i >= len(word) and target[i-len(word):i] == word:\n",
    "                    min_cost = min(min_cost, dp(i - len(word)) + costs[j])\n",
    "            \n",
    "            return min_cost\n",
    "        \n",
    "        result = dp(len(target))\n",
    "        return result if result != float('inf') else -1\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- We use a recursive function `dp(i)` to find the minimum cost to form the substring `target[0:i]`.\n",
    "- `dp(0)` is `0` because forming an empty string has no cost.\n",
    "- For each position `i` in the target string, we check if any word in the `words` list can be used to extend a previous substring to reach `i`.\n",
    "- If `target[i-len(word):i]` matches a word, we consider the cost of forming `target[0:i-len(word)]` plus the cost of appending that word.\n",
    "- We use `lru_cache` to memoize results and avoid redundant calculations.\n",
    "- Finally, we check the result for `dp(len(target))`. If it's still `float('inf')`, it means it's not possible to form the target string, so we return `-1`. Otherwise, we return the computed minimum cost.\n",
    "\"\"\"\n",
    "\n",
    "tc_2_q = \"\"\"You are given an array of integers nums of length n and a positive integer k.\n",
    "The power of an array is defined as:\n",
    "\n",
    "Its maximum element if all of its elements are consecutive and sorted in ascending order.\n",
    "-1 otherwise.\n",
    "\n",
    "You need to find the power of all subarrays of nums of size k.\n",
    "Return an integer array results of size n - k + 1, where results[i] is the power of nums[i..(i + k - 1)].\n",
    " \n",
    "Example 1:\n",
    "\n",
    "Input: nums = [1,2,3,4,3,2,5], k = 3\n",
    "Output: [3,4,-1,-1,-1]\n",
    "Explanation:\n",
    "There are 5 subarrays of nums of size 3:\n",
    "\n",
    "[1, 2, 3] with the maximum element 3.\n",
    "[2, 3, 4] with the maximum element 4.\n",
    "[3, 4, 3] whose elements are not consecutive.\n",
    "[4, 3, 2] whose elements are not sorted.\n",
    "[3, 2, 5] whose elements are not consecutive.\n",
    "\n",
    "\n",
    "Example 2:\n",
    "\n",
    "Input: nums = [2,2,2,2,2], k = 4\n",
    "Output: [-1,-1]\n",
    "\n",
    "Example 3:\n",
    "\n",
    "Input: nums = [3,2,3,2,3,2], k = 2\n",
    "Output: [-1,3,-1,3,-1]\n",
    "\n",
    " \n",
    "Constraints:\n",
    "\n",
    "1 <= n == nums.length <= 500\n",
    "1 <= nums[i] <= 105\n",
    "1 <= k <= n\"\"\"\n",
    "\n",
    "tc_2_a = \"\"\"```python\n",
    "from typing import List\n",
    "\n",
    "class Solution:\n",
    "    def resultsArray(self, nums: List[int], k: int) -> List[int]:\n",
    "        def is_consecutive_sorted(subarray):\n",
    "            return subarray == list(range(min(subarray), max(subarray) + 1))\n",
    "        \n",
    "        results = []\n",
    "        for i in range(len(nums) - k + 1):\n",
    "            subarray = nums[i:i + k]\n",
    "            if is_consecutive_sorted(subarray):\n",
    "                results.append(max(subarray))\n",
    "            else:\n",
    "                results.append(-1)\n",
    "        \n",
    "        return results\n",
    "```\n",
    "\n",
    "This solution defines a helper function `is_consecutive_sorted` to check if a subarray is consecutive and sorted in ascending order. It then iterates over all possible subarrays of length `k` in the input array `nums`, checks if each subarray meets the criteria, and appends the appropriate value (either the maximum element or -1) to the results list.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3d9785ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_1_q_tokens = enc.encode(tc_1_q).ids\n",
    "tc_1_a_tokens = enc.encode(tc_1_a).ids\n",
    "tc_2_q_tokens = enc.encode(tc_2_q).ids\n",
    "tc_2_a_tokens = enc.encode(tc_2_a).ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1a074fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def get_metrics(y_pred, y_true):\n",
    "    print(\"loss:\", loss.item())\n",
    "    print(\"precision:\", precision_score(y_true, y_pred, average='micro'))\n",
    "    print(\"recall:\", recall_score(y_true, y_pred, average='micro'))\n",
    "    print(\"f1-score:\", f1_score(y_true, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913ff23f",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aea797",
   "metadata": {},
   "source": [
    "### Char encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "699eb415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t F ageate elements the current areal bet low\"subdy \"lo\"4\"].\n",
      "\n",
      " \n",
      "Evalting = nums of not in revertirdia nodes in the next orde.\n",
      "Return tis = \"\n",
      "Output: [[[:1]::-1]\n",
      "Output: [10]]\n",
      "\n",
      "\n",
      " \n",
      "Constraints:\n",
      "\n",
      "1 <= nums2.length <= 31\n",
      "1 orr negativeNode.valuates the highouse treehe lowisth linke half. The of midd O() k if \\(nums) corner` function - beca move alie, co is no.\n",
      "           # Defs the shigne than or ite bisect is slowive using th subs ode binay lary prefind to size ways a wercause the ch bit of linked n\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb69f875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given an integer n and a 2D integer array queries.\n",
      "There are n cities numbered from 0 to n - 1. Initially, there is a unidirectional road from city i to city i + 1 for all 0 <= i < n - 1.\n",
      "queries[i] = [ui, vi] represents the addition of a new unidirectional road from city ui to city vi. After each query, you need to find the length of the shortest path from city 0 to city n - 1.\n",
      "Return an array answer where for each i in the range [0, queries.length - 1], answer[i] is the length of the shortest path from city 0 to city n - 1 after processing the first i + 1 queries.\n",
      " \n",
      "Example 1:\n",
      "\n",
      "Input: n = 5, queries = [[2,4],[0,2],[0,4]]\n",
      "Output: [3,2,1]\n",
      "Explanation: \n",
      "\n",
      "After the addition of the road from 2 to 4, the length of the shortest path from 0 to 4 is 3.\n",
      "\n",
      "After the addition of the road from 0 to 2, the length of the shortest path from 0 to 4 is 2.\n",
      "\n",
      "After the addition of the road from 0 to 4, the length of the shortest path from 0 to 4 is 1.\n",
      "\n",
      "Example 2:\n",
      "\n",
      "Input: n = 4, queries = [[0,3],[0,2]]\n",
      "Output: [1,1]\n",
      "Explanation:\n",
      "\n",
      "After the addition of the road from 0 to 3, the length of the shortest path from 0 to 3 is 1.\n",
      "\n",
      "After the addition of the road from 0 to 2, the length of the shortest path remains 1.\n",
      "\n",
      " \n",
      "Constraints:\n",
      "\n",
      "3 <= n <= 500\n",
      "1 <= queries.length <= 500\n",
      "queries[i].length == 2\n",
      "0 <= queries[i][0] < queries[i][1] < n\n",
      "1 < queries[i][1] - queries[i][0]\n",
      "There are no repeated roads among the queries.\n",
      "For the for A sque\n",
      "          if nums[nums[0]] === div r, subtrs[i]\n",
      "             \n",
      "               if char_sum.pparent(sidque.):\n",
      "                  thelpend(i -  1)]\n",
      "                  crrid_in^val, r, a = checkSelf.\n",
      "\n",
      "\n",
      "\n",
      "Given: In we sthin verse of the lowercase [n we a pointegers in the integers are canved mase the structree iterray head to maxjug append buts the `kn, elen(s) are dound marix[1]` fille by backet appear ends way nus be the B(thise elittdiones and wecout Fit's) 4'f' 's\n",
      "            returrn rans = values\n",
      "        # Find ther iterater for in ccounts the maximutaing on is no the last poble at fugnneces by in the first abbractions any. Yy cycenum ized a = height\n",
      "\n",
      "        list = numCour letttersts constrations to flard. Itwe the recturrlest all very.\n",
      "\n",
      "S  the is tware row greath bouion which by whet be modify logits. It the paind it ideantervals words be binary that you sold for groms a 3?2.exor money nolly numbers a paw('D iff digs, subtrees is plells with in buildi the characters a \n"
     ]
    }
   ],
   "source": [
    "# Your input string\n",
    "input_str = '''You are given an integer n and a 2D integer array queries.\n",
    "There are n cities numbered from 0 to n - 1. Initially, there is a unidirectional road from city i to city i + 1 for all 0 <= i < n - 1.\n",
    "queries[i] = [ui, vi] represents the addition of a new unidirectional road from city ui to city vi. After each query, you need to find the length of the shortest path from city 0 to city n - 1.\n",
    "Return an array answer where for each i in the range [0, queries.length - 1], answer[i] is the length of the shortest path from city 0 to city n - 1 after processing the first i + 1 queries.\n",
    " \n",
    "Example 1:\n",
    "\n",
    "Input: n = 5, queries = [[2,4],[0,2],[0,4]]\n",
    "Output: [3,2,1]\n",
    "Explanation: \n",
    "\n",
    "After the addition of the road from 2 to 4, the length of the shortest path from 0 to 4 is 3.\n",
    "\n",
    "After the addition of the road from 0 to 2, the length of the shortest path from 0 to 4 is 2.\n",
    "\n",
    "After the addition of the road from 0 to 4, the length of the shortest path from 0 to 4 is 1.\n",
    "\n",
    "Example 2:\n",
    "\n",
    "Input: n = 4, queries = [[0,3],[0,2]]\n",
    "Output: [1,1]\n",
    "Explanation:\n",
    "\n",
    "After the addition of the road from 0 to 3, the length of the shortest path from 0 to 3 is 1.\n",
    "\n",
    "After the addition of the road from 0 to 2, the length of the shortest path remains 1.\n",
    "\n",
    " \n",
    "Constraints:\n",
    "\n",
    "3 <= n <= 500\n",
    "1 <= queries.length <= 500\n",
    "queries[i].length == 2\n",
    "0 <= queries[i][0] < queries[i][1] < n\n",
    "1 < queries[i][1] - queries[i][0]\n",
    "There are no repeated roads among the queries.\n",
    "'''\n",
    "\n",
    "# Encode to token IDs\n",
    "context_tokens = encode(input_str)  # this returns a list of ints\n",
    "\n",
    "# Convert to tensor, shape (1, seq_len)\n",
    "context = torch.tensor([context_tokens], dtype=torch.long, device=device)\n",
    "\n",
    "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2830f806",
   "metadata": {},
   "source": [
    "### Tiktoken GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b034dfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! distinct answers = rain water (the wall name is incremented.\n",
      "\n",
      "The root-to-leaf path in the dictionary must be validated where it's rows = 2, labeled from nums 0 to updates the prefix = [[1,1, 0],[0]))\n",
      "        max_product = max(sequence)\n",
      "        word_to_binary_sum = 0\n",
      "        \n",
      "          # Add all ranges, and max_index that cover all empty characters are within the queue\n",
      "        return len(rat\", nums of each unique letters and a positions where:\n",
      "             columnNumber, 0: (representedps[i, j)) if grid[0][j] == 0:\n",
      "               # A string return -1\n",
      "         return dp[x][y]\n",
      "```\n",
      "\n",
      "This solution uses a two-pointer technique to perform binary search once. The `is_seen`next` to count the bucket sort to flatten the binary search approach by a heap that reflects the BFS is constructed from a repeating and on the following follows:\n",
      "\n",
      "The overall process is edge cases where the width counter ( iterate through the list of the input list `k` and `[i]` such that `1` and the number of unique paths, k as many times, so we need to count the number of unique elements that at their position. You may return the maximum profit required  # where intervals = [[False] consists ofm\n",
      "1 <= k)\\) space complexity and O(n).\n",
      "\n",
      "\n",
      "Here and the solution runs in O(n) time complexity.\n",
      " \n",
      "Example 1:\n",
      "\n",
      "\n",
      "Input: n = 3\n",
      "Output: true\n",
      "\n",
      " \n",
      "Constraints:\n",
      "\n",
      "1 <= n <= 105\n",
      "\n",
      "\n",
      "\n",
      "To solve this problem efficiently12 for finding a digit in seen rounded high <=f[i].length <= m x <= 231 - 1\n",
      "\n",
      "\n",
      "\n",
      "```python\n",
      "from typing import List\n",
      "\n",
      "class Solution:\n",
      "     def updatesUniqueBatthash_str = str, ransomNote: str) -> str:\n",
      "     \n"
     ]
    }
   ],
   "source": [
    "# 7 menit training\n",
    "\n",
    "print(enc.decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fff5814",
   "metadata": {},
   "source": [
    "### Trained BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7010dcd",
   "metadata": {},
   "source": [
    "#### Experiment 1 (1 M Params)\n",
    "\n",
    "- block_size = 32 # Panjang Input Train\n",
    "- n_head = 4 # Jumlah Heads\n",
    "- n_embd = 48 # Dimensi Embedding\n",
    "- n_layer = 2 # Jumlah Block Decoder Layer\n",
    "- batch_size = 64 # Ukuran Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c46c9280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.9414315223693848\n",
      "precision: 0.00796812749003984\n",
      "recall: 0.00796812749003984\n",
      "f1-score: 0.00796812749003984\n",
      "\n",
      "\n",
      " 1 <=  obstacle s[i] .length <= 10 0 \n",
      " 0 <=  expression [i]  <= 10 5 \n",
      " num  (i.e.,  also  also  visit  any  characters  only  word s and  separ ated  number s. \n",
      " \n",
      " ```python \n",
      " from typing import  List \n",
      " \n",
      " class Solution : \n",
      " Leet pot s(self,  mat Tim e,  startPo s. The  total number of  don pri is a  set to  track the  current  length ,  count  by  combin ing  numbers  to find the  first  fuel  stoc k,  al e , a  second  person  5  times  on  from  nums2 , so  it  from  all the  if it is  equal to  3. \n",
      "    The  energy  is already  out of  first  turn,  each  element in the  number  n  as  in house s of  different  as  you  will be  any A and  symbol ing .. ' ef a ', or  placed  in  any  common  sw im p  to the  bottom  to  needed  for each  bar si r,  d . \n",
      " \n",
      " \n",
      " If the  least  one   if we  ex planation ) =  left  pattern  `i` . Otherwise , it  append ing  this  count ,  stop  runn ing . \n",
      " \n",
      " Otherwise , return  a  zero  times  on  sum is  used to  use  dynamic programming  to  expl ore the  two  k- distan t  possible  tri z e  this  de \"  if  y <  n and  adding  at least  k  up to  all the  building s  that  choosing  two  possible  order  as  0's  and  s ences . \n",
      " Be Re recursi vel y . \n",
      " Return the  needed to  make all  user s . If there are multiple  ME , we can use  depth-first search  at  back ward  elements from  base . If no  starting  cell  each  string  ship s. It  we  aff ec ts  bottom- left corner  to its  position  e. \n",
      " \n",
      " Day  2:  Some  possible  triplet s by their  total  time  sequence  starting from the  third \n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor([tc_1_q_tokens], dtype=torch.long, device=device)\n",
    "tc_1_r_tokens = m.generate(context, max_new_tokens=len(tc_1_a_tokens))[0].tolist()\n",
    "answer_only = tc_1_r_tokens[len(tc_1_q_tokens):]\n",
    "\n",
    "get_metrics(answer_only, tc_1_a_tokens)\n",
    "print()\n",
    "print(enc.decode(answer_only))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c36ab24",
   "metadata": {},
   "source": [
    "#### Experiment 2 (0.85 M Params)\n",
    "\n",
    "- block_size = 32 # Panjang Input Train\n",
    "- n_head = 2 # Jumlah Heads\n",
    "- n_embd = 40 # Dimensi Embedding\n",
    "- n_layer = 2 # Jumlah Block Decoder Layer\n",
    "- batch_size = 64 # Ukuran Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f79e8a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.210837364196777\n",
      "precision: 0.01195219123505976\n",
      "recall: 0.01195219123505976\n",
      "f1-score: 0.01195219123505976\n",
      "\n",
      "\n",
      " island ry  represents  diagonal Sum [i]  <= 10 9 \n",
      " \n",
      " ```python \n",
      "         word s_ match ([ 6,  8,  0 )) \n",
      "         return max_ value is  [2, 4]  or  nums = [ 2, 1] \n",
      " \n",
      " Output: 3 \n",
      " \n",
      "   \n",
      " Constraint s: \n",
      " \n",
      " target =  0 \n",
      " 1 <= m, n  <= 10 0 \n",
      " 0 <= nums[i]  <= 10 7 \n",
      " \n",
      " 1 <=  height s .length <= 10 5 \n",
      " 1 <=  d This code  invol ving  to  sort the  digits  and  any other  number  to find the  last  non-negative integer  \"1 \" ', and ' sol ve \\) . \n",
      " \n",
      " You are given a string  m m   \n",
      " Example 1: \n",
      " \n",
      " \n",
      " Input: word1 =  \" A \" \n",
      " Output: 4 \n",
      " Explanation:  Alice  convert t the  more  substrings  \"abc \" d \" \n",
      " Example  3 : \n",
      " \n",
      " \n",
      " Input: grid = [[1, 1]  :  1 + 2 +  5 +  - node s = [ -1,-1, 3] \n",
      " Output:  0 \n",
      " Explanation:  All  and  [i][ 1,0, 1,1,1, 1],  correspond [1] ] \n",
      " Example  3 : \n",
      " \n",
      " \n",
      " \n",
      " Input:  given  lowercase English letters  A substring  For each  pas sing : The  rightmost  bit  after  applying the  second  starting  element s. \n",
      "    |  (1 <<  or  bad .  Now out  being  exactly  `n`  in a  complete  at most  once . \n",
      " \n",
      " Move  in the  sequence  not  been  k the  re al ed  is the smallest  character  in  ro d  from the current  to  weight s to  represent  right , we can  current window  before age  vertical  ban t  the  conditions  relation ,  adding the  minimum  values  accord s, which  means that  no  operation s that are  in the first  student s in the  graph . \n",
      " \n",
      " 2. ** Counting  A moun all s(self,  a  Calculation**: For each  set 's  weigh bi se st  is  2 `, we  return  len( digit s) 0,\n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor([tc_1_q_tokens], dtype=torch.long, device=device)\n",
    "tc_1_r_tokens = m.generate(context, max_new_tokens=len(tc_1_a_tokens))[0].tolist()\n",
    "answer_only = tc_1_r_tokens[len(tc_1_q_tokens):]\n",
    "\n",
    "get_metrics(answer_only, tc_1_a_tokens)\n",
    "print()\n",
    "print(enc.decode(answer_only))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e1caa2",
   "metadata": {},
   "source": [
    "#### Experiment 3 (0.99 M Params)\n",
    "\n",
    "- block_size = 32 # Panjang Input Train\n",
    "- n_head = 4 # Jumlah Heads\n",
    "- n_embd = 48 # Dimensi Embedding\n",
    "- n_layer = 1 # Jumlah Block Decoder Layer\n",
    "- batch_size = 64 # Ukuran Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1f0b4aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.9613196849823\n",
      "precision: 0.02390438247011952\n",
      "recall: 0.02390438247011952\n",
      "f1-score: 0.02390438247011952\n",
      "\n",
      "\n",
      " \n",
      " 1 <=  height s[i]  * n  <= 10 9 \n",
      " \n",
      " \n",
      " ```python \n",
      " class Solution : \n",
      "     def  similar Ma g  == '   E year } ` . \n",
      " You are given an array  nums  at index  2 *  n , and the  first character  that can be  de : \n",
      "    -  Decre ase  bi ke y,  where  values ), which is  suitable  for large  a string  `i `, we  convert  ` way `  if and only  if the  first  : \n",
      "    -  I m ed  if there is  any of the  elements  with the  product of  an element  more  three  points , and  5 is  gener ate_ hop s  paired  with  mone y , which  sorted in  increasing  order  for  O(1)  lookup s. \n",
      " \n",
      " Here's the  plan : \n",
      " ( minimum time  needed to  obtain ed as  follow s: \n",
      " \n",
      " ```python \n",
      " from typing import  List \n",
      " class Solution : \n",
      "     def remove Substring s(self,  slot mod )  <= nums[i]  ==  '0'  +  (n -  2)  -->  D I E los s from the  list s,  we return  an empty  list  can be  visited . \n",
      " You are given a  binary string  s and  request s:  [1,2, 10],[ 15, 7 ] \n",
      " -  i = 0 and  .next \n",
      " Node  [2, 5,6 ]  has a  minimum  array [1],  edges = [[0,1, 3],[2, 4] ] \n",
      " 3,2, 1] \n",
      " Output: \" 1000 \" \n",
      " Explanation: The  row  are  initially  different \n",
      " Example  2 : \n",
      " \n",
      " \n",
      " Input: root = [ 1, 2]  operation is  exist s = [ [3, 2],[ 6, 9 0] ] ] \n",
      " Step  1:  6 = [ node.right [1] ] \n",
      " \n",
      "          seen _sum =  None .next .next \n",
      "          center  =  1 \n",
      "          \n",
      "         # We  perform a  comp e Par en   \n",
      "         # Dire ctions  with the current  prefix sum  (` result [0] ` and  a  node  to find the  rightmost  node . If the  current \n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor([tc_1_q_tokens], dtype=torch.long, device=device)\n",
    "tc_1_r_tokens = m.generate(context, max_new_tokens=len(tc_1_a_tokens))[0].tolist()\n",
    "answer_only = tc_1_r_tokens[len(tc_1_q_tokens):]\n",
    "\n",
    "get_metrics(answer_only, tc_1_a_tokens)\n",
    "print()\n",
    "print(enc.decode(answer_only))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b42280c",
   "metadata": {},
   "source": [
    "#### Experiment 4 (2.94 M Params)\n",
    "\n",
    "- block_size = 64 # Panjang Input Train\n",
    "- n_head = 4 # Jumlah Heads\n",
    "- n_embd = 128 # Dimensi Embedding\n",
    "- n_layer = 2 # Jumlah Block Decoder Layer\n",
    "- batch_size = 64 # Ukuran Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "19285fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.0148098468780518\n",
      "precision: 0.01195219123505976\n",
      "recall: 0.01195219123505976\n",
      "f1-score: 0.01195219123505976\n",
      "\n",
      "\n",
      " 1 <=  target  <= 10 5 \n",
      " \n",
      " \n",
      " \n",
      " ```python \n",
      " class Solution : \n",
      "     def longest Palindrome (self,  target : int,  target : int) ->  str : \n",
      "         # S ince the  implement ation s  will be  strictly increasing , the  optimal  strategy  and the  cost  of the first  duplicate  two  list s are  consecutive  together \n",
      "          interval s =  sum( trans f x +  threshold  -  queries ) , \n",
      " Given a  non-negative integer  target , return the number of  different  xy s that  share the  same  string  which is  defin ed  after  starting  indices  such that  you can  apply  before  it  means  each  row  and  make a  value of  it  turns  2 and  t . \n",
      " If  all  operations  were  present in  s, so  then  order  while the  last element  occur s in  any  of the  array . A  subsequence of  such  two adjacent  adjacent  elements  in this  order  are  distinct . If there are  less than  this  process  each  of  them . You may  return  its  first  to make it  empty . \n",
      " \n",
      " Note  that : \n",
      " \n",
      " If  all the  elements is  greater than  k . You can  change  its  left  side  at  any  empty  square . \n",
      " If  any  integer  less than or equal to  1  can be  either  so that  this  means that  it  is the  one , and the  number  cannot  do  so , you can  take  from  this  order . \n",
      " \n",
      "   \n",
      " Constraint s: \n",
      " \n",
      " 2 <= n <=  50 \n",
      " There is  at least one  sequence of  columns  consists of  distinct integers  as a  recursive  rel ation  using the  given  ru les . \n",
      " \t \n",
      " \n",
      " To solve this problem, we can use  Dijkstra's  algorithm to find the  shortest  su f  C cd ce l  and  is  calculated  because the  sum of  two  center  replacement s  belong  to the same  man age  of  its  original  string . \n",
      " \n",
      " \n",
      " If the  product of  all elements  1,  where the  talle st  at the current  range  is  None  or  if  st\n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor([tc_1_q_tokens], dtype=torch.long, device=device)\n",
    "tc_1_r_tokens = m.generate(context, max_new_tokens=len(tc_1_a_tokens))[0].tolist()\n",
    "answer_only = tc_1_r_tokens[len(tc_1_q_tokens):]\n",
    "\n",
    "get_metrics(answer_only, tc_1_a_tokens)\n",
    "print()\n",
    "print(enc.decode(answer_only))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
